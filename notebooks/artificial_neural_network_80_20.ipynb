{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<b>Dependencies:</b> <br>\n",
    "    import pandas as pd <br>\n",
    "    import random <br><br>\n",
    "    from sklearn.preprocessing import StandardScaler <br>\n",
    "    from sklearn.model_selection import train_test_split <br>\n",
    "    from sklearn.metrics import accuracy_score <br>\n",
    "    from sklearn.metrics import precision_score <br>\n",
    "    from sklearn.metrics import recall_score <br>\n",
    "    from sklearn.metrics import f1_score <br>\n",
    "    import torch <br>\n",
    "    import torch.nn as nn <br>\n",
    "    import torch.nn.functional as F <br>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " ANN\n",
      "     epoch  Accuracy  Precision     Recall        f1\n",
      "0     100  0.737931    0.595238  0.543478  0.568182\n",
      "1     200  0.724138    0.575000  0.500000  0.534884\n",
      "2     300  0.710345    0.547619  0.500000  0.522727\n",
      "3     400  0.710345    0.547619  0.500000  0.522727\n",
      "4     500  0.703448    0.534884  0.500000  0.516854\n",
      "5     600  0.703448    0.534884  0.500000  0.516854\n",
      "6     700  0.703448    0.534884  0.500000  0.516854\n",
      "7     800  0.703448    0.534884  0.500000  0.516854\n",
      "8     900  0.710345    0.547619  0.500000  0.522727\n",
      "9    1000  0.710345    0.547619  0.500000  0.522727\n",
      "10   1100  0.710345    0.547619  0.500000  0.522727\n",
      "11   1200  0.710345    0.547619  0.500000  0.522727\n",
      "12   1300  0.710345    0.547619  0.500000  0.522727\n",
      "13   1400  0.703448    0.534884  0.500000  0.516854\n",
      "14   1500  0.696552    0.523810  0.478261  0.500000 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Best epoch 60-20-20\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Evaluation methods\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# tp: True Positive\n",
    "# fp: False Positive\n",
    "# tn: True Negative\n",
    "# fn: false negative\n",
    "\n",
    "# Number of correct predictions / Total number of predictions\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# tp / (tp + fp) -> Important when the cost of False Positive is high\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# tp / (tp + fn) -> Important when the cost of False Negative is high\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# (2* precision * recall) / (precision + recall) -> When looking for a balance between Precision and Recall AND\n",
    "#                                                   there is an uneven class distribution (large number of negatives)\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# ANN libraries\n",
    "import torch\n",
    "# Base class for all neural network modules\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Custom_ANN_Model(nn.Module):\n",
    "    \"\"\" Artificial Neural Network model used\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features=8, neurons_first_layer=20, neurons_second_layer=20, out_features=2):\n",
    "        \"\"\" Inits Custom_ANN_Model hidden layers.\n",
    "\n",
    "        :param in_features: int Number of input features\n",
    "        :params neurons_X_layer: int Number of neurons in the X hidden layer\n",
    "        :param out_features: int Number of output features\n",
    "        \"\"\"\n",
    "\n",
    "        super(Custom_ANN_Model, self).__init__()\n",
    "        # Definition of the two hidden layers\n",
    "        self.f_connected1 = nn.Linear(in_features=in_features, out_features=neurons_first_layer)\n",
    "        self.f_connected2 = nn.Linear(in_features=neurons_first_layer, out_features=neurons_second_layer)\n",
    "        self.out = nn.Linear(in_features=neurons_second_layer, out_features=out_features)\n",
    "\n",
    "    def forward(self, h):\n",
    "        \"\"\" Defines the computation performed at every call.\n",
    "        \"\"\"\n",
    "\n",
    "        # Layers activation\n",
    "        h = F.relu(self.f_connected1(h))\n",
    "        h = F.relu(self.f_connected2(h))\n",
    "        h = self.out(h)\n",
    "        return h\n",
    "\n",
    "\n",
    "# Using the 80% set for training (60%) and validation(%20), saving the remaining 20% for test\n",
    "diabetes_cleaned= pd.read_csv('../datasets/diabetes_train_data_80pc.csv')\n",
    "features = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness',\n",
    "            'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']\n",
    "X = diabetes_cleaned[features]\n",
    "y = diabetes_cleaned.Outcome\n",
    "accuracy = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "random_seed = 3\n",
    "\n",
    "# Standardization of the columns formed by different values (e.g. Age [21-81] and Glucose [44-199])\n",
    "# so that they can use a common scale\n",
    "scaler= StandardScaler()\n",
    "scaled= scaler.fit_transform(X)\n",
    "\n",
    "X_train,X_val,y_train,y_val= train_test_split(scaled, y, random_state=random_seed)\n",
    "# Lists to tensors\n",
    "X_train= torch.FloatTensor(X_train)\n",
    "X_val= torch.FloatTensor(X_val)\n",
    "y_train= torch.LongTensor(y_train.values)\n",
    "y_val= torch.LongTensor(y_val.values)\n",
    "\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "model=Custom_ANN_Model()\n",
    "\n",
    "# Common loss function used in classification tasks\n",
    "loss_function= nn.CrossEntropyLoss()\n",
    "# lr: Learning Rate\n",
    "optimizer= torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500]\n",
    "for epoch in epochs:\n",
    "    for i in range(1, epoch):\n",
    "        y_pred = model.forward(X_train)\n",
    "        loss = loss_function(y_pred,y_train)\n",
    "        # if i%10==1:\n",
    "            # print(\"Epoch number: {} \\t\\tLoss: {}\".format(i, loss.item()))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    predictions=[]\n",
    "    with torch.no_grad():\n",
    "        for i,data in enumerate(X_val):\n",
    "            y_pred = model(data)\n",
    "            predictions.append(y_pred.argmax().item())\n",
    "\n",
    "\n",
    "    accuracy.append(accuracy_score(y_val, predictions))\n",
    "    precision.append(precision_score(y_val, predictions))\n",
    "    recall.append(recall_score(y_val,predictions))\n",
    "    f1.append(f1_score(y_val,predictions))\n",
    "\n",
    "\n",
    "results_ANN = pd.DataFrame({'epoch': epochs,\n",
    "                            'Accuracy': accuracy,\n",
    "                            'Precision ': precision,\n",
    "                            'Recall': recall,\n",
    "                            'f1': f1,})\n",
    "\n",
    "print(\"\\n\\n\", \"ANN Validation\\n\", results_ANN, \"\\n\\n\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " ANN\n",
      "   Model  Accuracy  Precision     Recall        f1\n",
      "0   ANN  0.648276    0.517241  0.566038  0.540541 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ANN 80-20\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Evaluation methods\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# tp: True Positive\n",
    "# fp: False Positive\n",
    "# tn: True Negative\n",
    "# fn: false negative\n",
    "\n",
    "# Number of correct predictions / Total number of predictions\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# tp / (tp + fp) -> Important when the cost of False Positive is high\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# tp / (tp + fn) -> Important when the cost of False Negative is high\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# (2* precision * recall) / (precision + recall) -> When looking for a balance between Precision and Recall AND\n",
    "#                                                   there is an uneven class distribution (large number of negatives)\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# ANN libraries\n",
    "import torch\n",
    "# Base class for all neural network modules\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Custom_ANN_Model(nn.Module):\n",
    "    \"\"\" Artificial Neural Network model used\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features=8, neurons_first_layer=20, neurons_second_layer=20, out_features=2):\n",
    "        \"\"\" Inits Custom_ANN_Model hidden layers.\n",
    "\n",
    "        :param in_features: int Number of input features\n",
    "        :params neurons_X_layer: int Number of neurons in the X hidden layer\n",
    "        :param out_features: int Number of output features\n",
    "        \"\"\"\n",
    "\n",
    "        super(Custom_ANN_Model, self).__init__()\n",
    "        # Definition of the two hidden layers\n",
    "        self.f_connected1 = nn.Linear(in_features=in_features, out_features=neurons_first_layer)\n",
    "        self.f_connected2 = nn.Linear(in_features=neurons_first_layer, out_features=neurons_second_layer)\n",
    "        self.out = nn.Linear(in_features=neurons_second_layer, out_features=out_features)\n",
    "\n",
    "    def forward(self, h):\n",
    "        \"\"\" Defines the computation performed at every call.\n",
    "        \"\"\"\n",
    "\n",
    "        # Layers activation\n",
    "        h = F.relu(self.f_connected1(h))\n",
    "        h = F.relu(self.f_connected2(h))\n",
    "        h = self.out(h)\n",
    "        return h\n",
    "\n",
    "\n",
    "diabetes_cleaned= pd.read_csv('../datasets/diabetes_cleaned.csv')\n",
    "features = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness',\n",
    "            'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']\n",
    "X = diabetes_cleaned[features]\n",
    "y = diabetes_cleaned.Outcome\n",
    "random_seed = 3\n",
    "\n",
    "# Standardization of the columns formed by different values (e.g. Age [21-81] and Glucose [44-199])\n",
    "# so that they can use a common scale\n",
    "scaler= StandardScaler()\n",
    "scaled= scaler.fit_transform(X)\n",
    "\n",
    "X_train,X_val,y_train,y_val= train_test_split(scaled, y, test_size=0.2, random_state=random_seed)\n",
    "# Lists to tensors\n",
    "X_train= torch.FloatTensor(X_train)\n",
    "X_val= torch.FloatTensor(X_val)\n",
    "y_train= torch.LongTensor(y_train.values)\n",
    "y_val= torch.LongTensor(y_val.values)\n",
    "\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "model=Custom_ANN_Model()\n",
    "\n",
    "# Common loss function used in classification tasks\n",
    "loss_function= nn.CrossEntropyLoss()\n",
    "# lr: Learning Rate\n",
    "optimizer= torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 900\n",
    "for i in range(1, epochs):\n",
    "    y_pred = model.forward(X_train)\n",
    "    loss = loss_function(y_pred,y_train)\n",
    "    # if i%10==1:\n",
    "        # print(\"Epoch number: {} \\t\\tLoss: {}\".format(i, loss.item()))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "predictions=[]\n",
    "with torch.no_grad():\n",
    "    for i,data in enumerate(X_val):\n",
    "        y_pred = model(data)\n",
    "        predictions.append(y_pred.argmax().item())\n",
    "\n",
    "\n",
    "accuracy= accuracy_score(y_val, predictions)\n",
    "precision= precision_score(y_val, predictions)\n",
    "recall= recall_score(y_val,predictions)\n",
    "f1= f1_score(y_val,predictions)\n",
    "\n",
    "\n",
    "results_ANN = pd.DataFrame({'Model': [\"ANN\"],\n",
    "                            'Accuracy': [accuracy],\n",
    "                            'Precision ': [precision],\n",
    "                            'Recall': [recall],\n",
    "                            'f1': [f1],})\n",
    "\n",
    "print(\"\\n\\n\", \"ANN\\n\", results_ANN, \"\\n\\n\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " AVERAGE ANN\n",
      "   Model  Accuracy  Precision     Recall        f1\n",
      "0   ANN  0.701448    0.571257  0.571761  0.567911 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# AVERAGE ANN 80-20\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Evaluation methods\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# tp: True Positive\n",
    "# fp: False Positive\n",
    "# tn: True Negative\n",
    "# fn: false negative\n",
    "\n",
    "# Number of correct predictions / Total number of predictions\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# tp / (tp + fp) -> Important when the cost of False Positive is high\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# tp / (tp + fn) -> Important when the cost of False Negative is high\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# (2* precision * recall) / (precision + recall) -> When looking for a balance between Precision and Recall AND\n",
    "#                                                   there is an uneven class distribution (large number of negatives)\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# ANN libraries\n",
    "import torch\n",
    "# Base class for all neural network modules\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Custom_ANN_Model(nn.Module):\n",
    "    \"\"\" Artificial Neural Network model used\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features=8, neurons_first_layer=20, neurons_second_layer=20, out_features=2):\n",
    "        \"\"\" Inits Custom_ANN_Model hidden layers.\n",
    "\n",
    "        :param in_features: int Number of input features\n",
    "        :params neurons_X_layer: int Number of neurons in the X hidden layer\n",
    "        :param out_features: int Number of output features\n",
    "        \"\"\"\n",
    "\n",
    "        super(Custom_ANN_Model, self).__init__()\n",
    "        # Definition of the two hidden layers\n",
    "        self.f_connected1 = nn.Linear(in_features=in_features, out_features=neurons_first_layer)\n",
    "        self.f_connected2 = nn.Linear(in_features=neurons_first_layer, out_features=neurons_second_layer)\n",
    "        self.out = nn.Linear(in_features=neurons_second_layer, out_features=out_features)\n",
    "\n",
    "    def forward(self, h):\n",
    "        \"\"\" Defines the computation performed at every call.\n",
    "        \"\"\"\n",
    "\n",
    "        # Layers activation\n",
    "        h = F.relu(self.f_connected1(h))\n",
    "        h = F.relu(self.f_connected2(h))\n",
    "        h = self.out(h)\n",
    "        return h\n",
    "\n",
    "\n",
    "diabetes_cleaned= pd.read_csv('../datasets/diabetes_cleaned.csv')\n",
    "features = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness',\n",
    "            'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']\n",
    "X = diabetes_cleaned[features]\n",
    "y = diabetes_cleaned.Outcome\n",
    "accuracy = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "\n",
    "# Standardization of the columns formed by different values (e.g. Age [21-81] and Glucose [44-199])\n",
    "# so that they can use a common scale\n",
    "scaler= StandardScaler()\n",
    "scaled= scaler.fit_transform(X)\n",
    "\n",
    "# Common loss function used in classification tasks\n",
    "loss_function= nn.CrossEntropyLoss()\n",
    "epochs = 900\n",
    "\n",
    "times_repeated = 100\n",
    "for _ in range(times_repeated):\n",
    "    random_seed = random.randint(0, 1000)\n",
    "\n",
    "    X_train,X_val,y_train,y_val= train_test_split(scaled, y, test_size=0.2, random_state=random_seed)\n",
    "    # Lists to tensors\n",
    "    X_train= torch.FloatTensor(X_train)\n",
    "    X_val= torch.FloatTensor(X_val)\n",
    "    y_train= torch.LongTensor(y_train.values)\n",
    "    y_val= torch.LongTensor(y_val.values)\n",
    "\n",
    "    torch.manual_seed(random_seed)\n",
    "    model=Custom_ANN_Model()\n",
    "\n",
    "    # lr: Learning Rate\n",
    "    optimizer= torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    for i in range(1, epochs):\n",
    "        y_pred = model.forward(X_train)\n",
    "        loss = loss_function(y_pred,y_train)\n",
    "        # if i%10==1:\n",
    "            # print(\"Epoch number: {} \\t\\tLoss: {}\".format(i, loss.item()))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    predictions=[]\n",
    "    with torch.no_grad():\n",
    "        for i,data in enumerate(X_val):\n",
    "            y_pred = model(data)\n",
    "            predictions.append(y_pred.argmax().item())\n",
    "\n",
    "    accuracy.append(accuracy_score(y_val, predictions))\n",
    "    precision.append(precision_score(y_val, predictions))\n",
    "    recall.append(recall_score(y_val,predictions))\n",
    "    f1.append(f1_score(y_val,predictions))\n",
    "\n",
    "\n",
    "results_ANN = pd.DataFrame({'Model': [\"ANN\"],\n",
    "                            'Accuracy': [sum(accuracy)/len(accuracy)],\n",
    "                            'Precision ': [sum(precision)/len(precision)],\n",
    "                            'Recall': [sum(recall)/len(recall)],\n",
    "                            'f1': [sum(f1)/len(f1)],})\n",
    "\n",
    "print(\"\\n\\n\", \"AVERAGE ANN\\n\", results_ANN, \"\\n\\n\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " ANN with Feature Selection\n",
      "   Model  Accuracy  Precision     Recall        f1\n",
      "0   ANN  0.655172    0.525424  0.584906  0.553571 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ANN WITH FEATURE SELECTION 80-20\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Evaluation methods\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# tp: True Positive\n",
    "# fp: False Positive\n",
    "# tn: True Negative\n",
    "# fn: false negative\n",
    "\n",
    "# Number of correct predictions / Total number of predictions\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# tp / (tp + fp) -> Important when the cost of False Positive is high\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# tp / (tp + fn) -> Important when the cost of False Negative is high\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# (2* precision * recall) / (precision + recall) -> When looking for a balance between Precision and Recall AND\n",
    "#                                                   there is an uneven class distribution (large number of negatives)\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# ANN libraries\n",
    "import torch\n",
    "# Base class for all neural network modules\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Custom_ANN_Model(nn.Module):\n",
    "    \"\"\" Artificial Neural Network model used\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features=4, neurons_first_layer=20, neurons_second_layer=20, out_features=2):\n",
    "        \"\"\" Inits Custom_ANN_Model hidden layers.\n",
    "\n",
    "        :param in_features: int Number of input features\n",
    "        :params neurons_X_layer: int Number of neurons in the X hidden layer\n",
    "        :param out_features: int Number of output features\n",
    "        \"\"\"\n",
    "\n",
    "        super(Custom_ANN_Model, self).__init__()\n",
    "        # Definition of the two hidden layers\n",
    "        self.f_connected1 = nn.Linear(in_features=in_features, out_features=neurons_first_layer)\n",
    "        self.f_connected2 = nn.Linear(in_features=neurons_first_layer, out_features=neurons_second_layer)\n",
    "        self.out = nn.Linear(in_features=neurons_second_layer, out_features=out_features)\n",
    "\n",
    "    def forward(self, h):\n",
    "        \"\"\" Defines the computation performed at every call.\n",
    "        \"\"\"\n",
    "\n",
    "        # Layers activation\n",
    "        h = F.relu(self.f_connected1(h))\n",
    "        h = F.relu(self.f_connected2(h))\n",
    "        h = self.out(h)\n",
    "        return h\n",
    "\n",
    "\n",
    "diabetes_cleaned= pd.read_csv('../datasets/diabetes_cleaned.csv')\n",
    "selected_features = ['Glucose', 'BMI', 'DiabetesPedigreeFunction', 'Age']\n",
    "X = diabetes_cleaned[selected_features]\n",
    "y = diabetes_cleaned.Outcome\n",
    "random_seed = 3\n",
    "\n",
    "# Standardization of the columns formed by different values (e.g. Age [21-81] and Glucose [44-199])\n",
    "# so that they can use a common scale\n",
    "scaler= StandardScaler()\n",
    "scaled= scaler.fit_transform(X)\n",
    "\n",
    "X_train,X_val,y_train,y_val= train_test_split(scaled, y, test_size=0.2, random_state=random_seed)\n",
    "# Lists to tensors\n",
    "X_train= torch.FloatTensor(X_train)\n",
    "X_val= torch.FloatTensor(X_val)\n",
    "y_train= torch.LongTensor(y_train.values)\n",
    "y_val= torch.LongTensor(y_val.values)\n",
    "\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "model=Custom_ANN_Model()\n",
    "\n",
    "# Common loss function used in classification tasks\n",
    "loss_function= nn.CrossEntropyLoss()\n",
    "# lr: Learning Rate\n",
    "optimizer= torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 900\n",
    "for i in range(1, epochs):\n",
    "    y_pred = model.forward(X_train)\n",
    "    loss = loss_function(y_pred,y_train)\n",
    "    # if i%10==1:\n",
    "        # print(\"Epoch number: {} \\t\\tLoss: {}\".format(i, loss.item()))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "predictions=[]\n",
    "with torch.no_grad():\n",
    "    for i,data in enumerate(X_val):\n",
    "        y_pred = model(data)\n",
    "        predictions.append(y_pred.argmax().item())\n",
    "\n",
    "\n",
    "accuracy= accuracy_score(y_val, predictions)\n",
    "precision= precision_score(y_val, predictions)\n",
    "recall= recall_score(y_val,predictions)\n",
    "f1= f1_score(y_val,predictions)\n",
    "\n",
    "\n",
    "results_ANN = pd.DataFrame({'Model': [\"ANN\"],\n",
    "                            'Accuracy': [accuracy],\n",
    "                            'Precision ': [precision],\n",
    "                            'Recall': [recall],\n",
    "                            'f1': [f1],})\n",
    "\n",
    "print(\"\\n\\n\", \"ANN with Feature Selection\\n\", results_ANN, \"\\n\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " AVERAGE ANN with Feature Selection\n",
      "   Model  Accuracy  Precision     Recall        f1\n",
      "0   ANN  0.707586    0.575975  0.572835  0.571576 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# AVERAGE ANN with Feature Selection\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Evaluation methods\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# tp: True Positive\n",
    "# fp: False Positive\n",
    "# tn: True Negative\n",
    "# fn: false negative\n",
    "\n",
    "# Number of correct predictions / Total number of predictions\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# tp / (tp + fp) -> Important when the cost of False Positive is high\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# tp / (tp + fn) -> Important when the cost of False Negative is high\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# (2* precision * recall) / (precision + recall) -> When looking for a balance between Precision and Recall AND\n",
    "#                                                   there is an uneven class distribution (large number of negatives)\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# ANN libraries\n",
    "import torch\n",
    "# Base class for all neural network modules\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Custom_ANN_Model(nn.Module):\n",
    "    \"\"\" Artificial Neural Network model used\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features=4, neurons_first_layer=20, neurons_second_layer=20, out_features=2):\n",
    "        \"\"\" Inits Custom_ANN_Model hidden layers.\n",
    "\n",
    "        :param in_features: int Number of input features\n",
    "        :params neurons_X_layer: int Number of neurons in the X hidden layer\n",
    "        :param out_features: int Number of output features\n",
    "        \"\"\"\n",
    "\n",
    "        super(Custom_ANN_Model, self).__init__()\n",
    "        # Definition of the two hidden layers\n",
    "        self.f_connected1 = nn.Linear(in_features=in_features, out_features=neurons_first_layer)\n",
    "        self.f_connected2 = nn.Linear(in_features=neurons_first_layer, out_features=neurons_second_layer)\n",
    "        self.out = nn.Linear(in_features=neurons_second_layer, out_features=out_features)\n",
    "\n",
    "    def forward(self, h):\n",
    "        \"\"\" Defines the computation performed at every call.\n",
    "        \"\"\"\n",
    "\n",
    "        # Layers activation\n",
    "        h = F.relu(self.f_connected1(h))\n",
    "        h = F.relu(self.f_connected2(h))\n",
    "        h = self.out(h)\n",
    "        return h\n",
    "\n",
    "\n",
    "diabetes_cleaned= pd.read_csv('../datasets/diabetes_cleaned.csv')\n",
    "selected_features = ['Glucose', 'BMI', 'DiabetesPedigreeFunction', 'Age']\n",
    "X = diabetes_cleaned[selected_features]\n",
    "y = diabetes_cleaned.Outcome\n",
    "accuracy = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "\n",
    "# Standardization of the columns formed by different values (e.g. Age [21-81] and Glucose [44-199])\n",
    "# so that they can use a common scale\n",
    "scaler= StandardScaler()\n",
    "scaled= scaler.fit_transform(X)\n",
    "\n",
    "# Common loss function used in classification tasks\n",
    "loss_function= nn.CrossEntropyLoss()\n",
    "epochs = 900\n",
    "\n",
    "times_repeated = 100\n",
    "for _ in range(times_repeated):\n",
    "    random_seed = random.randint(0, 1000)\n",
    "\n",
    "    X_train,X_val,y_train,y_val= train_test_split(scaled, y, test_size=0.2, random_state=random_seed)\n",
    "    # Lists to tensors\n",
    "    X_train= torch.FloatTensor(X_train)\n",
    "    X_val= torch.FloatTensor(X_val)\n",
    "    y_train= torch.LongTensor(y_train.values)\n",
    "    y_val= torch.LongTensor(y_val.values)\n",
    "\n",
    "    torch.manual_seed(random_seed)\n",
    "    model=Custom_ANN_Model()\n",
    "\n",
    "    # lr: Learning Rate\n",
    "    optimizer= torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    for i in range(1, epochs):\n",
    "        y_pred = model.forward(X_train)\n",
    "        loss = loss_function(y_pred,y_train)\n",
    "        # if i%10==1:\n",
    "            # print(\"Epoch number: {} \\t\\tLoss: {}\".format(i, loss.item()))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    predictions=[]\n",
    "    with torch.no_grad():\n",
    "        for i,data in enumerate(X_val):\n",
    "            y_pred = model(data)\n",
    "            predictions.append(y_pred.argmax().item())\n",
    "\n",
    "    accuracy.append(accuracy_score(y_val, predictions))\n",
    "    precision.append(precision_score(y_val, predictions))\n",
    "    recall.append(recall_score(y_val,predictions))\n",
    "    f1.append(f1_score(y_val,predictions))\n",
    "\n",
    "\n",
    "results_ANN = pd.DataFrame({'Model': [\"ANN\"],\n",
    "                            'Accuracy': [sum(accuracy)/len(accuracy)],\n",
    "                            'Precision ': [sum(precision)/len(precision)],\n",
    "                            'Recall': [sum(recall)/len(recall)],\n",
    "                            'f1': [sum(f1)/len(f1)],})\n",
    "\n",
    "print(\"\\n\\n\", \"AVERAGE ANN with Feature Selection\\n\", results_ANN, \"\\n\\n\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " ANN with Data Augmentation\n",
      "   Model  Accuracy  Precision     Recall        f1\n",
      "0   ANN  0.703448    0.595745  0.538462  0.565657 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ANN WITH DATA AUGMENTATION 80-20\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Evaluation methods\n",
    "\n",
    "# tp: True Positive\n",
    "# fp: False Positive\n",
    "# tn: True Negative\n",
    "# fn: false negative\n",
    "\n",
    "# Number of correct predictions / Total number of predictions\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# tp / (tp + fp) -> Important when the cost of False Positive is high\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# tp / (tp + fn) -> Important when the cost of False Negative is high\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# (2* precision * recall) / (precision + recall) -> When looking for a balance between Precision and Recall AND\n",
    "#                                                   there is an uneven class distribution (large number of negatives)\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# ANN libraries\n",
    "import torch\n",
    "# Base class for all neural network modules\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Custom_ANN_Model(nn.Module):\n",
    "    \"\"\" Artificial Neural Network model used\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features=8, neurons_first_layer=20, neurons_second_layer=20, out_features=2):\n",
    "        \"\"\" Inits Custom_ANN_Model hidden layers.\n",
    "\n",
    "        :param in_features: int Number of input features\n",
    "        :params neurons_X_layer: int Number of neurons in the X hidden layer\n",
    "        :param out_features: int Number of output features\n",
    "        \"\"\"\n",
    "\n",
    "        super(Custom_ANN_Model, self).__init__()\n",
    "        # Definition of the two hidden layers\n",
    "        self.f_connected1 = nn.Linear(in_features=in_features, out_features=neurons_first_layer)\n",
    "        self.f_connected2 = nn.Linear(in_features=neurons_first_layer, out_features=neurons_second_layer)\n",
    "        self.out = nn.Linear(in_features=neurons_second_layer, out_features=out_features)\n",
    "\n",
    "    def forward(self, h):\n",
    "        \"\"\" Defines the computation performed at every call.\n",
    "        \"\"\"\n",
    "\n",
    "        # Layers activation\n",
    "        h = F.relu(self.f_connected1(h))\n",
    "        h = F.relu(self.f_connected2(h))\n",
    "        h = self.out(h)\n",
    "        return h\n",
    "\n",
    "\n",
    "diabetes_da_training = pd.read_csv('../datasets/diabetes_train_data_80pc_100times_10.csv')\n",
    "diabetes_da_test = pd.read_csv('../datasets/diabetes_test_data_20pc.csv')\n",
    "features = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness',\n",
    "            'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']\n",
    "X_train = diabetes_da_training[features]\n",
    "y_train = diabetes_da_training.Outcome\n",
    "X_test = diabetes_da_test[features]\n",
    "y_test = diabetes_da_test.Outcome\n",
    "random_seed = 3\n",
    "\n",
    "# Standardization of the columns formed by different values (e.g. Age [21-81] and Glucose [44-199])\n",
    "# so that they can use a common scale\n",
    "scaler= StandardScaler()\n",
    "X_train= scaler.fit_transform(X_train)\n",
    "X_test= scaler.fit_transform(X_test)\n",
    "\n",
    "\n",
    "# Lists to tensors\n",
    "X_train= torch.FloatTensor(X_train)\n",
    "X_val= torch.FloatTensor(X_test)\n",
    "y_train= torch.LongTensor(y_train.values)\n",
    "y_val= torch.LongTensor(y_test.values)\n",
    "\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "model=Custom_ANN_Model()\n",
    "\n",
    "# Common loss function used in classification tasks\n",
    "loss_function= nn.CrossEntropyLoss()\n",
    "# lr: Learning Rate\n",
    "optimizer= torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 900\n",
    "for i in range(1, epochs):\n",
    "    y_pred = model.forward(X_train)\n",
    "    loss = loss_function(y_pred,y_train)\n",
    "    # if i%10==1:\n",
    "        # print(\"Epoch number: {} \\t\\tLoss: {}\".format(i, loss.item()))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "predictions=[]\n",
    "with torch.no_grad():\n",
    "    for i,data in enumerate(X_val):\n",
    "        y_pred = model(data)\n",
    "        predictions.append(y_pred.argmax().item())\n",
    "\n",
    "\n",
    "accuracy= accuracy_score(y_val, predictions)\n",
    "precision= precision_score(y_val, predictions)\n",
    "recall= recall_score(y_val,predictions)\n",
    "f1= f1_score(y_val,predictions)\n",
    "\n",
    "\n",
    "results_ANN = pd.DataFrame({'Model': [\"ANN\"],\n",
    "                            'Accuracy': [accuracy],\n",
    "                            'Precision ': [precision],\n",
    "                            'Recall': [recall],\n",
    "                            'f1': [f1],})\n",
    "\n",
    "print(\"\\n\\n\", \"ANN with Data Augmentation\\n\", results_ANN, \"\\n\\n\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " ANN with Feature Selection and Data Augmentation\n",
      "   Model  Accuracy  Precision     Recall        f1\n",
      "0   ANN  0.751724    0.690476  0.557692  0.617021 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ANN WITH FEATURE SELECTION AND DATA AUGMENTATION 80-20\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Evaluation methods\n",
    "\n",
    "# tp: True Positive\n",
    "# fp: False Positive\n",
    "# tn: True Negative\n",
    "# fn: false negative\n",
    "\n",
    "# Number of correct predictions / Total number of predictions\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# tp / (tp + fp) -> Important when the cost of False Positive is high\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# tp / (tp + fn) -> Important when the cost of False Negative is high\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# (2* precision * recall) / (precision + recall) -> When looking for a balance between Precision and Recall AND\n",
    "#                                                   there is an uneven class distribution (large number of negatives)\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# ANN libraries\n",
    "import torch\n",
    "# Base class for all neural network modules\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Custom_ANN_Model(nn.Module):\n",
    "    \"\"\" Artificial Neural Network model used\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features=4, neurons_first_layer=20, neurons_second_layer=20, out_features=2):\n",
    "        \"\"\" Inits Custom_ANN_Model hidden layers.\n",
    "\n",
    "        :param in_features: int Number of input features\n",
    "        :params neurons_X_layer: int Number of neurons in the X hidden layer\n",
    "        :param out_features: int Number of output features\n",
    "        \"\"\"\n",
    "\n",
    "        super(Custom_ANN_Model, self).__init__()\n",
    "        # Definition of the two hidden layers\n",
    "        self.f_connected1 = nn.Linear(in_features=in_features, out_features=neurons_first_layer)\n",
    "        self.f_connected2 = nn.Linear(in_features=neurons_first_layer, out_features=neurons_second_layer)\n",
    "        self.out = nn.Linear(in_features=neurons_second_layer, out_features=out_features)\n",
    "\n",
    "    def forward(self, h):\n",
    "        \"\"\" Defines the computation performed at every call.\n",
    "        \"\"\"\n",
    "\n",
    "        # Layers activation\n",
    "        h = F.relu(self.f_connected1(h))\n",
    "        h = F.relu(self.f_connected2(h))\n",
    "        h = self.out(h)\n",
    "        return h\n",
    "\n",
    "\n",
    "diabetes_da_training = pd.read_csv('../datasets/diabetes_train_data_80pc_100times_10.csv')\n",
    "diabetes_da_test = pd.read_csv('../datasets/diabetes_test_data_20pc.csv')\n",
    "selected_features = ['Glucose', 'BMI', 'DiabetesPedigreeFunction', 'Age']\n",
    "X_train = diabetes_da_training[selected_features]\n",
    "y_train = diabetes_da_training.Outcome\n",
    "X_test = diabetes_da_test[selected_features]\n",
    "y_test = diabetes_da_test.Outcome\n",
    "random_seed = 3\n",
    "\n",
    "# Standardization of the columns formed by different values (e.g. Age [21-81] and Glucose [44-199])\n",
    "# so that they can use a common scale\n",
    "scaler= StandardScaler()\n",
    "X_train= scaler.fit_transform(X_train)\n",
    "X_test= scaler.fit_transform(X_test)\n",
    "\n",
    "\n",
    "# Lists to tensors\n",
    "X_train= torch.FloatTensor(X_train)\n",
    "X_val= torch.FloatTensor(X_test)\n",
    "y_train= torch.LongTensor(y_train.values)\n",
    "y_val= torch.LongTensor(y_test.values)\n",
    "\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "model=Custom_ANN_Model()\n",
    "\n",
    "# Common loss function used in classification tasks\n",
    "loss_function= nn.CrossEntropyLoss()\n",
    "# lr: Learning Rate\n",
    "optimizer= torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 900\n",
    "for i in range(1, epochs):\n",
    "    y_pred = model.forward(X_train)\n",
    "    loss = loss_function(y_pred,y_train)\n",
    "    # if i%10==1:\n",
    "        # print(\"Epoch number: {} \\t\\tLoss: {}\".format(i, loss.item()))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "predictions=[]\n",
    "with torch.no_grad():\n",
    "    for i,data in enumerate(X_val):\n",
    "        y_pred = model(data)\n",
    "        predictions.append(y_pred.argmax().item())\n",
    "\n",
    "\n",
    "accuracy= accuracy_score(y_val, predictions)\n",
    "precision= precision_score(y_val, predictions)\n",
    "recall= recall_score(y_val,predictions)\n",
    "f1= f1_score(y_val,predictions)\n",
    "\n",
    "\n",
    "results_ANN = pd.DataFrame({'Model': [\"ANN\"],\n",
    "                            'Accuracy': [accuracy],\n",
    "                            'Precision ': [precision],\n",
    "                            'Recall': [recall],\n",
    "                            'f1': [f1],})\n",
    "\n",
    "print(\"\\n\\n\", \"ANN with Feature Selection and Data Augmentation\\n\", results_ANN, \"\\n\\n\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " ANN with Feature Selection and Data Augmentation. 4 Layers of 10 neurons \n",
      "   Model  Accuracy  Precision     Recall        f1\n",
      "0   ANN  0.786207    0.723404  0.653846  0.686869 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ANN WITH FEATURE SELECTION AND DATA AUGMENTATION 80-20; FOUR LAYERS\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Evaluation methods\n",
    "\n",
    "# tp: True Positive\n",
    "# fp: False Positive\n",
    "# tn: True Negative\n",
    "# fn: false negative\n",
    "\n",
    "# Number of correct predictions / Total number of predictions\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# tp / (tp + fp) -> Important when the cost of False Positive is high\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# tp / (tp + fn) -> Important when the cost of False Negative is high\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# (2* precision * recall) / (precision + recall) -> When looking for a balance between Precision and Recall AND\n",
    "#                                                   there is an uneven class distribution (large number of negatives)\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# ANN libraries\n",
    "import torch\n",
    "# Base class for all neural network modules\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Report generation\n",
    "from python_project.test_recorder import TestRecorder\n",
    "\n",
    "\n",
    "class Custom_ANN_Model(nn.Module):\n",
    "    \"\"\" Artificial Neural Network model used\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features=4, neurons_first_layer=10, neurons_second_layer=10, neurons_third_layer=10,\n",
    "                 neurons_fourth_layer=10, out_features=2):\n",
    "        \"\"\" Inits Custom_ANN_Model hidden layers.\n",
    "\n",
    "        :param in_features: int Number of input features\n",
    "        :params neurons_X_layer: int Number of neurons in the X hidden layer\n",
    "        :param out_features: int Number of output features\n",
    "        \"\"\"\n",
    "\n",
    "        super(Custom_ANN_Model, self).__init__()\n",
    "        # Definition of the three hidden layers\n",
    "        self.f_connected1 = nn.Linear(in_features=in_features, out_features=neurons_first_layer)\n",
    "        self.f_connected2 = nn.Linear(in_features=neurons_first_layer, out_features=neurons_second_layer)\n",
    "        self.f_connected3 = nn.Linear(in_features=neurons_second_layer, out_features=neurons_third_layer)\n",
    "        self.f_connected4 = nn.Linear(in_features=neurons_third_layer, out_features=neurons_fourth_layer)\n",
    "        self.out = nn.Linear(in_features=neurons_fourth_layer, out_features=out_features)\n",
    "\n",
    "    def forward(self, h):\n",
    "        \"\"\" Defines the computation performed at every call.\n",
    "        \"\"\"\n",
    "\n",
    "        # Layers activation\n",
    "        h = F.relu(self.f_connected1(h))\n",
    "        h = F.relu(self.f_connected2(h))\n",
    "        h = F.relu(self.f_connected3(h))\n",
    "        h = F.relu(self.f_connected3(h))\n",
    "        h = self.out(h)\n",
    "        return h\n",
    "\n",
    "\n",
    "diabetes_da_training = pd.read_csv('../datasets/diabetes_train_data_80pc_100times_10.csv')\n",
    "diabetes_da_test = pd.read_csv('../datasets/diabetes_test_data_20pc.csv')\n",
    "selected_features = ['Glucose', 'BMI', 'DiabetesPedigreeFunction', 'Age']\n",
    "X_train = diabetes_da_training[selected_features]\n",
    "y_train = diabetes_da_training.Outcome\n",
    "X_test = diabetes_da_test[selected_features]\n",
    "y_test = diabetes_da_test.Outcome\n",
    "random_seed = 3\n",
    "\n",
    "# Standardization of the columns formed by different values (e.g. Age [21-81] and Glucose [44-199])\n",
    "# so that they can use a common scale\n",
    "scaler= StandardScaler()\n",
    "X_train= scaler.fit_transform(X_train)\n",
    "X_test= scaler.fit_transform(X_test)\n",
    "\n",
    "\n",
    "# Lists to tensors\n",
    "X_train= torch.FloatTensor(X_train)\n",
    "X_val= torch.FloatTensor(X_test)\n",
    "y_train= torch.LongTensor(y_train.values)\n",
    "y_val= torch.LongTensor(y_test.values)\n",
    "\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "model=Custom_ANN_Model()\n",
    "\n",
    "# Common loss function used in classification tasks\n",
    "loss_function= nn.CrossEntropyLoss()\n",
    "# lr: Learning Rate\n",
    "optimizer= torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 900\n",
    "for i in range(1, epochs):\n",
    "    y_pred = model.forward(X_train)\n",
    "    loss = loss_function(y_pred,y_train)\n",
    "    # if i%10==1:\n",
    "        # print(\"Epoch number: {} \\t\\tLoss: {}\".format(i, loss.item()))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "predictions=[]\n",
    "with torch.no_grad():\n",
    "    for i,data in enumerate(X_val):\n",
    "        y_pred = model(data)\n",
    "        predictions.append(y_pred.argmax().item())\n",
    "\n",
    "\n",
    "accuracy= accuracy_score(y_val, predictions)\n",
    "precision= precision_score(y_val, predictions)\n",
    "recall= recall_score(y_val,predictions)\n",
    "f1= f1_score(y_val,predictions)\n",
    "\n",
    "\n",
    "results_ANN = pd.DataFrame({'Model': [\"ANN\"],\n",
    "                            'Accuracy': [accuracy],\n",
    "                            'Precision ': [precision],\n",
    "                            'Recall': [recall],\n",
    "                            'f1': [f1],})\n",
    "\n",
    "test_description = \"ANN with Feature Selection and Data Augmentation. 4 Layers of 10 neurons\"\n",
    "print(\"\\n\\n\", test_description, \"\\n\", results_ANN, \"\\n\\n\")\n",
    "\n",
    "results = TestRecorder(description=test_description, accuracy=accuracy, precision=precision, recall=recall, f1=f1)\n",
    "\n",
    "results.data_to_json()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " ANN with Feature Selection and Data Augmentation. 8 Layers of 10 neurons \n",
      "   Model  Accuracy  Precision     Recall    f1\n",
      "0   ANN  0.765517      0.6875  0.634615  0.66 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ANN WITH FEATURE SELECTION AND DATA AUGMENTATION 80-20; EIGHT LAYERS\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Evaluation methods\n",
    "\n",
    "# tp: True Positive\n",
    "# fp: False Positive\n",
    "# tn: True Negative\n",
    "# fn: false negative\n",
    "\n",
    "# Number of correct predictions / Total number of predictions\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# tp / (tp + fp) -> Important when the cost of False Positive is high\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# tp / (tp + fn) -> Important when the cost of False Negative is high\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# (2* precision * recall) / (precision + recall) -> When looking for a balance between Precision and Recall AND\n",
    "#                                                   there is an uneven class distribution (large number of negatives)\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# ANN libraries\n",
    "import torch\n",
    "# Base class for all neural network modules\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Report generation\n",
    "from python_project.test_recorder import TestRecorder\n",
    "\n",
    "\n",
    "class Custom_ANN_Model(nn.Module):\n",
    "    \"\"\" Artificial Neural Network model used\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features=4, neurons_first_layer=10, neurons_second_layer=10, neurons_third_layer=10,\n",
    "                 neurons_fourth_layer=10, neurons_fifth_layer=10, neurons_sixth_layer=10, neurons_seventh_layer=10,\n",
    "                 neurons_eighth_layer=10, out_features=2):\n",
    "        \"\"\" Inits Custom_ANN_Model hidden layers.\n",
    "\n",
    "        :param in_features: int Number of input features\n",
    "        :params neurons_X_layer: int Number of neurons in the X hidden layer\n",
    "        :param out_features: int Number of output features\n",
    "        \"\"\"\n",
    "\n",
    "        super(Custom_ANN_Model, self).__init__()\n",
    "        # Definition of the three hidden layers\n",
    "        self.f_connected1 = nn.Linear(in_features=in_features, out_features=neurons_first_layer)\n",
    "        self.f_connected2 = nn.Linear(in_features=neurons_first_layer, out_features=neurons_second_layer)\n",
    "        self.f_connected3 = nn.Linear(in_features=neurons_second_layer, out_features=neurons_third_layer)\n",
    "        self.f_connected4 = nn.Linear(in_features=neurons_third_layer, out_features=neurons_fourth_layer)\n",
    "        self.f_connected5 = nn.Linear(in_features=neurons_fourth_layer, out_features=neurons_fifth_layer)\n",
    "        self.f_connected6 = nn.Linear(in_features=neurons_fifth_layer, out_features=neurons_sixth_layer)\n",
    "        self.f_connected7 = nn.Linear(in_features=neurons_sixth_layer, out_features=neurons_seventh_layer)\n",
    "        self.f_connected8 = nn.Linear(in_features=neurons_seventh_layer, out_features=neurons_eighth_layer)\n",
    "        self.out = nn.Linear(in_features=neurons_eighth_layer, out_features=out_features)\n",
    "\n",
    "    def forward(self, h):\n",
    "        \"\"\" Defines the computation performed at every call.\n",
    "        \"\"\"\n",
    "\n",
    "        # Layers activation\n",
    "        h = F.relu(self.f_connected1(h))\n",
    "        h = F.relu(self.f_connected2(h))\n",
    "        h = F.relu(self.f_connected3(h))\n",
    "        h = F.relu(self.f_connected3(h))\n",
    "        h = self.out(h)\n",
    "        return h\n",
    "\n",
    "\n",
    "diabetes_da_training = pd.read_csv('../datasets/diabetes_train_data_80pc_100times_10.csv')\n",
    "diabetes_da_test = pd.read_csv('../datasets/diabetes_test_data_20pc.csv')\n",
    "selected_features = ['Glucose', 'BMI', 'DiabetesPedigreeFunction', 'Age']\n",
    "X_train = diabetes_da_training[selected_features]\n",
    "y_train = diabetes_da_training.Outcome\n",
    "X_test = diabetes_da_test[selected_features]\n",
    "y_test = diabetes_da_test.Outcome\n",
    "random_seed = 3\n",
    "\n",
    "# Standardization of the columns formed by different values (e.g. Age [21-81] and Glucose [44-199])\n",
    "# so that they can use a common scale\n",
    "scaler= StandardScaler()\n",
    "X_train= scaler.fit_transform(X_train)\n",
    "X_test= scaler.fit_transform(X_test)\n",
    "\n",
    "\n",
    "# Lists to tensors\n",
    "X_train= torch.FloatTensor(X_train)\n",
    "X_val= torch.FloatTensor(X_test)\n",
    "y_train= torch.LongTensor(y_train.values)\n",
    "y_val= torch.LongTensor(y_test.values)\n",
    "\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "model=Custom_ANN_Model()\n",
    "\n",
    "# Common loss function used in classification tasks\n",
    "loss_function= nn.CrossEntropyLoss()\n",
    "# lr: Learning Rate\n",
    "optimizer= torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 900\n",
    "for i in range(1, epochs):\n",
    "    y_pred = model.forward(X_train)\n",
    "    loss = loss_function(y_pred,y_train)\n",
    "    # if i%10==1:\n",
    "        # print(\"Epoch number: {} \\t\\tLoss: {}\".format(i, loss.item()))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "predictions=[]\n",
    "with torch.no_grad():\n",
    "    for i,data in enumerate(X_val):\n",
    "        y_pred = model(data)\n",
    "        predictions.append(y_pred.argmax().item())\n",
    "\n",
    "\n",
    "accuracy= accuracy_score(y_val, predictions)\n",
    "precision= precision_score(y_val, predictions)\n",
    "recall= recall_score(y_val,predictions)\n",
    "f1= f1_score(y_val,predictions)\n",
    "\n",
    "\n",
    "results_ANN = pd.DataFrame({'Model': [\"ANN\"],\n",
    "                            'Accuracy': [accuracy],\n",
    "                            'Precision ': [precision],\n",
    "                            'Recall': [recall],\n",
    "                            'f1': [f1],})\n",
    "\n",
    "test_description = \"ANN with Feature Selection and Data Augmentation. 8 Layers of 10 neurons\"\n",
    "print(\"\\n\\n\", test_description, \"\\n\", results_ANN, \"\\n\\n\")\n",
    "\n",
    "results = TestRecorder(description=test_description, accuracy=accuracy, precision=precision, recall=recall, f1=f1)\n",
    "\n",
    "results.data_to_json()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}